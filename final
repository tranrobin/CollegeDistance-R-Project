---
title: "**CollegeDistance dataset: Effect Of Background Factors On Individuals' Education**"
author: "Robin Tran"
date: "May 3, 2022"
output:
  pdf_document: default
  word_document: default
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)
```

```{r, message = FALSE}
library(AER)
library(dplyr) # functions like summarize
library(ggplot2) # for making plots
library(readr)
library(gridExtra)
library(GGally)
library(gmodels)
```

```{r}
data("CollegeDistance")
```

**Introduction**

This paper constructs statistical models to identify two main problems of interest, using the CollegeDistance dataset which first appeared in Rouse C.E’s journal (1995) on the effects of community colleges on educational attainment. In their journal, Rouse discussed the effect of college access on the educational achievement of an individual, by analyzing data collected about members of the high school senior class in different areas. In this paper, the focus will be on how an individual’s background can affect their education level, considering factors such as distance from home to the nearest 4-year college, ethnicity, and family income. The main hypotheses are:

(1) Distance from home to a 4-year college of a high school student does affect the number of education years they receive.

(2) Ethnicity of a high school student does affect the number of education years they receive.

(3) Family income of a high school student does affect the number of education years they receive.

This paper will use all subsets regression methods to come up with the most effective linear regression model to test the hypotheses. Transformation will be applied to satisfy multiple linear regression conditions as much as possible. Lastly, results from the tests will be stated and concluded in text. 


**Variables selection**

The CollegeDistance dataset contains 14 variables, including information about high school senior individuals, as well as information about the areas they were living in. In this paper, we will discuss gender, ethnicity, score (base year composite test score), fcollege (whether the father is a college graduate), mcollege (whether the mother is a college graduate), distance (distance from 4-year college), income (whether the family income is above $25000 per year), home (whether family income above USD 25,000 per year), urban (whether the school is in urban area), unemp (county unemployment rate in 1980), wage (state hourly wage in manufacturing in 1980), tuition (average state 4-year college tuition), region (West or other), and years of education.

```{r,results='hide',fig.keep='all'}
library(GGally)
ggpairs(CollegeDistance %>% select(gender, ethnicity, score, fcollege, mcollege, distance, income, education)) + 
  ggtitle("GGPairs ") +
  theme_bw() 
ggpairs(CollegeDistance %>% select(urban, unemp, wage, tuition, region, home, education)) +
  ggtitle("GGPairs") +
  theme_bw()
```

**Summary Statistics for Numerical Variables**

```{r, include=FALSE}
library(reshape2)
library(dplyr)
CollegeDistance_num <- CollegeDistance %>%
dplyr::select(score, distance, unemp, wage, tuition, education)
CollegeDistance_melt <- melt(CollegeDistance_num, id.vars = c()) 
head(CollegeDistance_melt)
```

```{r, include=FALSE}
library(dplyr) 

CollegeDistance_sumstats <- CollegeDistance_melt %>%
  group_by(variable) %>%
  summarize(Mean = mean(value), SD = sd(value), Min = min(value), Max = max(value))
CollegeDistance_sumstats
```

```{r}
options(knitr.table.format = "latex", knitr.kable.NA = "")
```

```{r}
library(kableExtra)

kable2 <- function(data, ...) {
  knitr::kable(data, ..., booktabs = TRUE, escape = FALSE, digits = 3) %>%
    kable_styling(position = "center", latex_options = "HOLD_position") }
kable2(CollegeDistance_sumstats)
```


**Model assumptions**

Firstly, the model with all variables will be fitted, along with a summary of the model. We will check the assumptions about multiple linear regression of this model. Independent observations might not be satisfied in this case when data was collected in nested structure – schools, counties, and states. Students from the same high school might have similar characteristics to each other, students from the same area (county, state) have the same values of average wage, etc. There the observations might not be independent. Also, the data was collected as a survey – a convenient method. For further analysis, we will assume that independence is satisfied, but it is noted that there is a problem of nested effect in this dataset.

```{r}
allvariables_fit <- lm(data = CollegeDistance, education ~ gender + ethnicity + score + fcollege + mcollege + home + urban + unemp + wage + distance + tuition + income + region)
summary(allvariables_fit)
```

Plots showing the relationship between numerical explanatory variables and the response variables education are included for a clearer view.

```{r, echo=FALSE,results='hide',fig.keep='all'}
p1 <- ggplot(data = CollegeDistance, aes(x = distance, y = education)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()
```

```{r, echo=FALSE,results='hide',fig.keep='all'}
p2 <- ggplot(data = CollegeDistance, aes(x = score, y = education)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()
```

```{r, echo=FALSE,results='hide',fig.keep='all'}
p3 <- ggplot(data = CollegeDistance, aes(x = unemp, y = education)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()
```

```{r, echo=FALSE,results='hide',fig.keep='all'}
p4 <- ggplot(data = CollegeDistance, aes(x = wage, y = education)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()
```

```{r, echo=FALSE,results='hide',fig.keep='all'}
p5 <- ggplot(data = CollegeDistance, aes(x = tuition, y = education)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()
```

```{r}
grid.arrange(p1, p2, p3, p4, p5, ncol = 2)
```

There are linear relationships between the numerical explanatory variable (distance, score, unemp, wage, tuition) and the response variable (education). Another limitation of this model is that education is not technically a continuous numerical variable, or can be categorized as a categorical variable. Therefore, the graphs are not ideal, and the linear relationship is not clear for education ~ wage, education ~ unemp, and education ~ tuition plots.

```{r, echo=FALSE,results='hide',fig.keep='all'}
CollegeDistance <- CollegeDistance %>%
  mutate(
    residual = residuals(allvariables_fit)
  )
ggpairs(CollegeDistance %>% select(score, unemp, distance, wage, tuition, residual)) +
  ggtitle("Residuals ~ Response")
  theme_bw()
```

Residuals are quite normally distributed, but can still be improved. We proceed by checking the equal standard deviation of response for all values for our numerical explanatory variables. Points scatter around line 0.0 without a particular pattern, but it still looks problematic. One approach is to apply a transformation to the variables; in this case, we can use log transformation.

```{r}
CollegeDistance <- CollegeDistance %>%
  mutate(
    log_education = log(education),
    log_score = log(score),
    log_distance = log(distance+1),
    log_wage = log(wage),
    log_tuition = log(tuition),
    log_unemp = log(unemp)
  )
```

We fit the model again with the transformed variables and view its summary.

```{r}
allvariables_fit <- lm(data = CollegeDistance, log_education ~ gender + ethnicity + log_score + fcollege + mcollege + home + urban + log_unemp + log_wage + log_distance + log_tuition + income + region)
summary(allvariables_fit)
```

```{r, echo=FALSE,results='hide',fig.keep='all'}
library(GGally)
ggpairs(CollegeDistance %>% select(gender, ethnicity, log_score, fcollege, mcollege, log_distance, income, log_education)) + 
  ggtitle("GGPairs After Log Transformation") +
  theme_bw()
ggpairs(CollegeDistance %>% select(urban, log_unemp, log_wage, log_tuition, region, home, log_education)) +
  ggtitle("GGPairs After Log Transformation") +
  theme_bw()
```

The linear relationships between the numerical explanatory variable (log_distance, log_score, log_unemp, log_wage, log_tution) and the response variable (log_education) are improved, but there is still problem of categorical response variable.

```{r}
CollegeDistance <- CollegeDistance %>%
  mutate(
    residual = residuals(allvariables_fit)
  )
```

```{r, echo=FALSE,results='hide',fig.keep='all'}
ggpairs(CollegeDistance %>% select(log_score, log_distance, log_tuition, log_unemp, log_wage, residual)) +
  theme_bw()
```

After the log transformation on score, distance, tuition, unemp, wage, and education, the residual plot looks more bell-curved, and the equal standard deviation of response assumption has improved (it can be seen in the explanatory ~ residual plots). The last condition that requires considering is influential points and leverage, which can be tested using diagnostic plots, including leverage plot, studentized residuals plot, and Cook’s distance plot. 

```{r}
CollegeDistance <- CollegeDistance %>%
  mutate(
    obs_index = row_number(),
    h = hatvalues(allvariables_fit),
    studres = rstudent(allvariables_fit),
    D = cooks.distance(allvariables_fit)
  )
```

```{r, echo=FALSE,results='hide',fig.keep='all'}
ggplot(data = CollegeDistance, aes(x = obs_index, y = h)) +
  geom_point() +
  geom_hline(yintercept = 2*15/nrow(CollegeDistance)) +
  ggtitle("Leverage") +
  theme_bw()
```

```{r, echo=FALSE,results='hide',fig.keep='all'}
ggplot(data = CollegeDistance, aes(x = obs_index, y = studres)) +
  geom_point() +
  ggtitle("Studentized Residuals") +
  geom_hline(yintercept = 0) +
  theme_bw()
```

```{r, echo=FALSE,results='hide',fig.keep='all'}
ggplot(data = CollegeDistance, aes(x = obs_index, y = D)) +
  geom_point() +
  ggtitle("Cook's Distance") +
  theme_bw()
```
As observed in the diagnostic plots, no point specifically stands out at an influential point. We can proceed with the model selection.


**Model selection**

For model selection, this paper will use all subsets regression and determine the model that contains both our variables of interest and the smallest BIC (Schwarz’s Bayesian Information Criterion). 

```{r, echo=FALSE,results='hide',fig.keep='all'}
library(leaps)
CollegeDistance_model1 <- regsubsets(data = CollegeDistance, log_education ~ gender + ethnicity + log_score + fcollege + mcollege + home + urban + log_unemp + log_wage + log_distance + log_tuition + income + region)
plot(CollegeDistance_model1) 
```

```{r}
summary(CollegeDistance_model1)
```

```{r, echo=FALSE,results='hide',fig.keep='all'}
vis_bic1 <- data.frame(Model=1:8, BIC = summary(CollegeDistance_model1)$bic)

ggplot(data = vis_bic1, aes(x = Model, y = BIC)) +
  geom_point(aes(color = BIC < -1400), size = 3) +
  geom_line() +
  theme_bw() +
  theme(legend.position = "none")
```

```{r}
summary(CollegeDistance_model1)$bic
```

According to the BIC plot, Model 6, Model 7, and Model 8 have roughly similar performances.

Model 6: ethnicity, log_score, fcollege, mcollege, income (BIC = -1403.094)

Model 7: ethnicity, log_score, fcollege, mcollege, log_distance, income (BIC = -1405.389)

Model 8: gender, ethnicity, log_score, fcollege, mcollege, log_distance, income (BIC = -1404.415)

Model 7 has the lowest BIC and also includes all of the variables we are interested in. Therefore, we will select Model 7 for further analysis. Using all subsets regression, we can see that home, urban, unemp, wage, tuition, and region variables are not significant for the model. On the other hand, the variables of our interest - ethnicity, score, fcollege, mcollege, and income are significant and consistent among 3 considered models with the lowest BIC. Distance appeared in 2 out of 3 considered models.

After considering the assumptions and model selection, we come up with a model which presumably satisfies the conditions for multiple linear regression, as well as includes the variables of interest.

```{r}
college_fit <- lm(data = CollegeDistance, log_education ~ ethnicity + log_score + fcollege + mcollege + income + log_distance)
summary(college_fit)
```

The equation for the model selected:

$\hat{\mu}$ = 1.376957 + 0.024572 $\times$ ethnicityafam + 0.024077 $\times$ ethnicityhispanic +  0.311417 $\times$ log_score + 0.038819 $\times$ fcollegeyes + 0.027997 $\times$ mcollegeyes + 0.026479 $\times$ incomehigh - 0.008774 $\times$ log_distance

ethnicityafam = 1 if ethnicity = afam, 0 otherwise.

ethnicityhispanic = 1 if ethnicity = hispanic, 0 otherwise.

log_score = log base year composite test score.

fcollegeyes = 1 if fcollege = yes, 0 otherwise.
            
mcollegeyes = 1 if mcollege = yes, 0 otherwise.
            
incomehigh = 1 if income = high, 0 otherwise.
          
log_distance = log distance from home to a 4-year college.

A ggpairs plot for selected model is provided below.

```{r, echo=FALSE,results='hide',fig.keep='all'}
ggpairs(CollegeDistance %>% select(ethnicity, log_score, fcollege, mcollege, log_distance, income, log_education)) +
  ggtitle("GGPairs For Selected Model")
  theme_bw()
```


**Hypotheses**

The first relationship we are interested in from this model is the effect of distance from a high school student’s home to a college on the number of education years they receive. This is also one focus of Rouse C.E when they first used the dataset and discussed the effect of college accessibility on education opportunities. In this paper, we will use F-test to see if log_distance has significant effect on education.

$H_0: \beta_0{distance} = 0$

$H_A: \beta_0{distance} \neq 0$

We will fit a model without the log_distance variable, and use ANOVA method on the full model and the reduced model.

```{r}
model_without_distance <- lm(log_education ~ ethnicity + log_score + fcollege + mcollege + income, data = CollegeDistance)
anova(model_without_distance, college_fit)
```

The p-value for this test is 0.001048, which says there is strong evidence against the null hypothesis that distance has no effect on education. The F-statistic is large and positive, which supports the argument that distance is a contributor of education.

Another hypothesis we are interested in is whether ethnicity affects the number of years of education a high school student achieve. We will also use F-test in this case.

$H_0: \mu_{afam} = \mu_{hispanic} = \mu_{other}$ 

There is no difference between the number of years of education among the ethnic groups. 

$H_A:$ At least one is different. There is difference between the years of education among the ethnic groups.

```{r}
model_without_ethnicity <- lm(log_education ~ log_score + fcollege + mcollege + log_distance + income, data = CollegeDistance)
anova(model_without_ethnicity, college_fit)
```
The p-value for this test is 1.542e-10, which is proof of very strong evidence against the null hypothesis that ethnicity has no effect on education. The F-statistic is large and positive, which supports the argument that ethnicity affects the number of years of education a high school student in the given population receive.

Whether an individual student's family income is high or low can affect the number of years of education a student can receive. F-test will be applied to see the relationship.

```{r}
model_without_income <- lm(log_education ~ ethnicity + log_score + fcollege + mcollege + log_distance, data = CollegeDistance)
anova(model_without_income, college_fit)
```

The p-value for this test is 3.665e-12, which is proof of very strong evidence against the null hypothesis that ethnicity has no effect on education. The F-statistic is large and positive, which supports the argument that family income affects the number of years of education a high school student in the given population receive.


**Confidence Intervals For fcollege and mcollege**

Having considered other factors of an student's background, such as ethnicity, income, it can be beneficial to see how mother's education level and father's education level can affect a student's education.

```{r}
confint(college_fit)
```

```{r}
exp(0.02983532)
exp(0.047802990)
exp(0.01789440)
exp(0.038099084)
```

We are 95% confident that for high school students whose father was a college graduate and whose father was not, students whose father was a college graduate receive between 1.03028 and 1.048964 more years of education, on average, holding other variables constant.

Similarly, we are 95% confident that for high school students whose mother was a college graduate and whose mother was not, students whose mother was a college graduate receive between 1.018055 and 1.038834 more years of education, on average, holding other variables constant.


**Results/Conclusions**

The analysis mainly focuses on how different aspects of an individual high school student’s background can affect their education level (indicated by years), and the significance of these aspects on the response variable. From all subsets regression application, we were able to determine the most efficient model that presumably satisfies the multiple linear regression conditions except for independence, which is a limitation of this model that will be discussed later.

From the model we selected, we can conclude that the students’ distance from a 4-year college will affect people’s years of education in populations similar to our study. Distance can serve as a natural experiment in a variety of applications (Rouse C.E, 1980). As the distance increased, the number of years of education for a student decreased. This can also apply in today’s world when many people are disconnected from higher education because of the lack of physical educational institutions (Victoria Rosenboom & Kristin Blagg, 2018) in the local area. 

The results also demonstrated that other background factors like ethnicity, family income, whether that individual’s mother went to college, and whether that individual’s father went to college are also significant contributors to educational achievement. If policymakers aim to improve people’s education rate/levels, it is helpful to provide more educational accessibility to some college-desert areas and provide supportive schemes for students with low family incomes. The focus should also be on racial equality in the educational system. 

**Limitations of model and future work**

As discussed earlier in the paper, this model as well as this dataset have several limitations that can affect the credibility of the results. First of all, we employed a categorical variable as the main response variable. 

When the response variable is categorical, a standard linear regression model can't be used, but we can use logistic regression models instead. Furthermore, the data was collected in a nested structure, which leads to dependence among the observations. In this case, one method is to use mixed-effects models, which allow both fixed and random effects, and are used when there is a non-independence in the data. The data was collected through a survey, a convenient method, which cannot ensure the credibility of the data. For future work, data collectors can choose populations randomly from random areas to achieve impartial results.
